# Chinese_interview_dataset: A High-Quality Chinese Interview Question Dataset

[![Hugging Face Datasets](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Datasets-yellow)](https://huggingface.co/datasets?search=zhangjun640/Chinese_interview)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

Hello everyone, I'm excited to introduce the **High-Quality Chinese Interview Dataset** that I have compiled and open-sourced.

This is a comprehensive dataset designed to support fine-tuning Large Language Models (LLMs), interview preparation, and Natural Language Processing research. All data is sourced from top-tier Chinese job-seeking and technical communication platforms like **Nowcoder (ç‰›å®¢ç½‘) and CSDN**, ensuring the authenticity, timeliness, and broad coverage of the questions.

---

### Dataset Links

To meet the needs of various computational resources and application scenarios, I have provided three versions of the dataset with different sizes:

* **Small(16.1K)**: 
    * [ğŸ¤— zhangjun640/Chinese_interview_small](https://huggingface.co/datasets/zhangjun640/Chinese_interview_small)
* **Medium(59K)**:
    * [ğŸ¤— zhangjun640/Chinese_interview_medium](https://huggingface.co/datasets/zhangjun640/Chinese_interview_medium)
* **Large(158K)**:
    * [ğŸ¤— zhangjun640/Chinese_interview_large](https://huggingface.co/datasets/zhangjun640/Chinese_interview_large)

### âœ¨ Key Features

* **Driven by Real Scenarios**: All question-answer pairs are meticulously collected and curated from real interview scenarios and technical discussions, avoiding "toy data" to address real-world challenges.
* **Multiple Scales**: Available in Small, Medium, and Large versions, allowing users to flexibly choose based on their computing power, time constraints, and model size. Whether for quick validation or pursuing maximum performance, there is a suitable version.
* **Broad Domain Coverage**: The dataset covers core knowledge points across popular technical fields such as Computer Science, Software Engineering, Data Science, and Algorithms.
* **Optimized for Chinese**: The content is natively in Chinese, conforming to Chinese linguistic context and technical expression habits, making it excellent corpus for fine-tuning Chinese language models.

### Data Format

The dataset follows the standard instruction fine-tuning format, containing fields like `instruction`, `input`, and `output`. This format can be seamlessly integrated into most mainstream LLM fine-tuning frameworks.

Here is a data sample:

```json
{
  "instruction": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯TCPçš„â€œä¸‰æ¬¡æ¡æ‰‹â€è¿‡ç¨‹ï¼Ÿ (Please explain the TCP 'Three-Way Handshake' process?)",
  "input": "",
  "output": "TCPçš„â€œä¸‰æ¬¡æ¡æ‰‹â€æ˜¯å»ºç«‹ä¸€ä¸ªTCPè¿æ¥æ—¶ï¼Œå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´æ€»å…±å‘é€ä¸‰ä¸ªåŒ…çš„ç¡®è®¤è¿‡ç¨‹ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š..."
}
```

### How to Use

**Note:** Accessing this dataset may require you to be logged into your Hugging Face account. You can log in via the command line:
```bash
huggingface-cli login
```

#### Using the `datasets` Library (Recommended)
This is the standard way to load and interact with the dataset for training models.

```python
from datasets import load_dataset

# Login using e.g. `huggingface-cli login` to access this dataset
ds = load_dataset("zhangjun640/Chinese_interview_small")

# Print dataset information
print(ds)

# View the first sample
print(ds['train'][0])
```

#### Using `pandas` to Load Raw Files
This method is useful for quick data analysis or if you want to load a specific file directly into a pandas DataFrame.

```python
import pandas as pd

# Define the path to a specific file in the dataset repository
file_path = "hf://datasets/zhangjun640/Chinese_interview_small/qa_dataset_part2_new.json"

# Login using e.g. `huggingface-cli login` to access this dataset
df = pd.read_json(file_path)

# Display the first few rows of the DataFrame
print(df.head())
```

### ğŸŒŸ Use Cases

* **LLM Fine-tuning**: Use as a high-quality instruction dataset to fine-tune Chinese language models.
* **Interview Preparation Tools**: Develop AI interviewers, intelligent chatbots, etc.
* **Information Retrieval and Q&A Systems**: Build an intelligent system capable of accurately answering technical interview questions.
* **NLP Academic Research**: For research in areas such as dialogue systems and text generation.

### Citation

If you use this dataset in your research or work, please consider citing it:

```bibtex
@misc{zhangjun640_chinese_interview,
  author = {zhangjun640},
  title = {Chinese Interview Dataset},
  year = {2025},
  publisher = {Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{[https://huggingface.co/datasets/zhangjun640/Chinese_interview_large](https://huggingface.co/datasets/zhangjun640/Chinese_interview_large)}},
}
```

### License

This dataset is licensed under the [Apache License 2.0](https://opensource.org/licenses/Apache-2.0).

---
---

# Chinese_interview_dataset: é«˜è´¨é‡ä¸­æ–‡é¢è¯•é—®é¢˜æ•°æ®é›†

[![Hugging Face Datasets](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Datasets-yellow)](https://huggingface.co/datasets?search=zhangjun640/Chinese_interview)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

å¤§å®¶å¥½ï¼Œéå¸¸è£å¹¸å‘å¤§å®¶ä»‹ç»æˆ‘æ•´ç†å¹¶å¼€æºçš„**é«˜è´¨é‡ä¸­æ–‡é¢è¯•é—®é¢˜æ•°æ®é›† (Chinese Interview Dataset)**ã€‚

è¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒã€é¢è¯•å‡†å¤‡ã€ä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶æä¾›æ”¯æŒçš„ç»¼åˆæ€§æ•°æ®é›†ã€‚æ‰€æœ‰æ•°æ®å‡æºè‡ªä¸­æ–‡äº’è”ç½‘å¤´éƒ¨æ±‚èŒä¸æŠ€æœ¯äº¤æµå¹³å°ï¼Œå¦‚**ç‰›å®¢ç½‘ã€CSDN**ç­‰ï¼Œç¡®ä¿äº†é—®é¢˜çš„çœŸå®æ€§ã€æ—¶æ•ˆæ€§å’Œè¦†ç›–é¢ã€‚

---

### æ•°æ®é›†é“¾æ¥

ä¸ºäº†æ»¡è¶³ä¸åŒè®¡ç®—èµ„æºå’Œåº”ç”¨åœºæ™¯çš„éœ€æ±‚ï¼Œæˆ‘æä¾›äº†ä¸‰ç§ä¸åŒè§„æ¨¡çš„æ•°æ®é›†ï¼š

* **Small (å°å‹ç‰ˆ)**: 
    * [ğŸ¤— zhangjun640/Chinese_interview_small](https://huggingface.co/datasets/zhangjun640/Chinese_interview_small)
* **Medium (ä¸­å‹ç‰ˆ)**:
    * [ğŸ¤— zhangjun640/Chinese_interview_medium](https://huggingface.co/datasets/zhangjun640/Chinese_interview_medium)
* **Large (å¤§å‹ç‰ˆ)**:
    * [ğŸ¤— zhangjun640/Chinese_interview_large](https://huggingface.co/datasets/zhangjun640/Chinese_interview_large)

### âœ¨ æ•°æ®é›†ç‰¹è‰²

* **çœŸå®åœºæ™¯é©±åŠ¨**: æ‰€æœ‰é—®ç­”å¯¹å‡ä»çœŸå®çš„é¢è¯•åœºæ™¯å’ŒæŠ€æœ¯è®¨è®ºä¸­ç²¾å¿ƒæ”¶é›†å’Œæ•´ç†ï¼Œæ‹’ç»â€œç©å…·æ•°æ®â€ï¼Œç›´é¢çœŸå®ä¸–ç•Œçš„æŒ‘æˆ˜ã€‚
* **å¤šè§„æ¨¡é€‰æ‹©**: æä¾› Small, Medium, Large ä¸‰ç§ç‰ˆæœ¬ï¼Œæ–¹ä¾¿ç”¨æˆ·æ ¹æ®è‡ªèº«ç®—åŠ›ã€æ—¶é—´å’Œæ¨¡å‹è§„æ¨¡çµæ´»é€‰æ‹©ã€‚
* **é¢†åŸŸè¦†ç›–å¹¿æ³›**: æ•°æ®é›†æ¶µç›–äº†è®¡ç®—æœºç§‘å­¦ã€è½¯ä»¶å·¥ç¨‹ã€æ•°æ®ç§‘å­¦ã€ç®—æ³•ç­‰å¤šä¸ªçƒ­é—¨æŠ€æœ¯é¢†åŸŸçš„æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚
* **ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–**: æ•°æ®å†…å®¹åŸç”Ÿä¸ºä¸­æ–‡ï¼Œç¬¦åˆä¸­æ–‡è¯­å¢ƒå’ŒæŠ€æœ¯è¡¨è¾¾ä¹ æƒ¯ï¼Œæ˜¯å¾®è°ƒä¸­æ–‡è¯­è¨€æ¨¡å‹çš„ç»ä½³è¯­æ–™ã€‚

### æ•°æ®æ ¼å¼

æœ¬æ•°æ®é›†éµå¾ªæ ‡å‡†çš„æŒ‡ä»¤å¾®è°ƒæ ¼å¼ï¼ŒåŒ…å« `instruction`, `input`, å’Œ `output` ç­‰å­—æ®µã€‚è¿™ç§æ ¼å¼å¯ä»¥æ— ç¼å¯¹æ¥åˆ°å¤§å¤šæ•°ä¸»æµçš„ LLM å¾®è°ƒæ¡†æ¶ä¸­ã€‚

ä¸€ä¸ªæ•°æ®æ ·ä¾‹å¦‚ä¸‹ï¼š

```json
{
  "instruction": "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯TCPçš„â€œä¸‰æ¬¡æ¡æ‰‹â€è¿‡ç¨‹ï¼Ÿ",
  "input": "",
  "output": "TCPçš„â€œä¸‰æ¬¡æ¡æ‰‹â€æ˜¯å»ºç«‹ä¸€ä¸ªTCPè¿æ¥æ—¶ï¼Œå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´æ€»å…±å‘é€ä¸‰ä¸ªåŒ…çš„ç¡®è®¤è¿‡ç¨‹ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š..."
}
```

### å¦‚ä½•ä½¿ç”¨

**è¯·æ³¨æ„ï¼š** è®¿é—®æ­¤æ•°æ®é›†å¯èƒ½éœ€è¦æ‚¨ç™»å½• Hugging Face è´¦æˆ·ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åœ¨ç»ˆç«¯ç™»å½•ï¼š
```bash
huggingface-cli login
```

#### ä½¿ç”¨ `datasets` åº“ (æ¨è)
è¿™æ˜¯åŠ è½½å’Œå¤„ç†æ•°æ®é›†ä»¥ç”¨äºæ¨¡å‹è®­ç»ƒçš„æ ‡å‡†æ–¹æ³•ã€‚

```python
from datasets import load_dataset

# ä½¿ç”¨ huggingface-cli login ç­‰æ–¹å¼ç™»å½•åï¼Œå³å¯è®¿é—®æ­¤æ•°æ®é›†
ds = load_dataset("zhangjun640/Chinese_interview_small")

# æ‰“å°æ•°æ®é›†ä¿¡æ¯
print(ds)

# æŸ¥çœ‹ç¬¬ä¸€æ¡æ ·æœ¬
print(ds['train'][0])
```

#### ä½¿ç”¨ `pandas` åŠ è½½åŸå§‹æ–‡ä»¶
å¦‚æœæ‚¨æƒ³è¿›è¡Œå¿«é€Ÿçš„æ•°æ®åˆ†æï¼Œæˆ–å¸Œæœ›å°†æŸä¸ªç‰¹å®šæ–‡ä»¶ç›´æ¥åŠ è½½åˆ° pandas DataFrame ä¸­ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ–¹æ³•ã€‚

```python
import pandas as pd

# å®šä¹‰æ•°æ®é›†ä¸­æŸä¸ªç‰¹å®šæ–‡ä»¶çš„è·¯å¾„
file_path = "hf://datasets/zhangjun640/Chinese_interview_small/qa_dataset_part2_new.json"

# ä½¿ç”¨ huggingface-cli login ç­‰æ–¹å¼ç™»å½•åï¼Œå³å¯è®¿é—®æ­¤æ•°æ®é›†
df = pd.read_json(file_path)

# æ˜¾ç¤º DataFrame çš„å‰å‡ è¡Œ
print(df.head())
```

### ğŸŒŸ åº”ç”¨åœºæ™¯

* **LLM å¾®è°ƒ**: ä½œä¸ºé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œç”¨äºå¾®è°ƒä¸­æ–‡è¯­è¨€æ¨¡å‹ã€‚
* **é¢è¯•å‡†å¤‡å·¥å…·**: å¼€å‘AIé¢è¯•å®˜ã€æ™ºèƒ½é—®ç­”æœºå™¨äººç­‰ã€‚
* **ä¿¡æ¯æ£€ç´¢ä¸é—®ç­”ç³»ç»Ÿ**: æ„å»ºä¸€ä¸ªèƒ½å¤Ÿç²¾å‡†å›ç­”æŠ€æœ¯é¢è¯•é—®é¢˜çš„æ™ºèƒ½ç³»ç»Ÿã€‚
* **NLPå­¦æœ¯ç ”ç©¶**: ç”¨äºå¯¹è¯ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆç­‰ç›¸å…³é¢†åŸŸçš„ç ”ç©¶ã€‚

### å¼•ç”¨

å¦‚æœæ‚¨åœ¨æ‚¨çš„ç ”ç©¶æˆ–å·¥ä½œä¸­ä½¿ç”¨äº†æœ¬æ•°æ®é›†ï¼Œè¯·è€ƒè™‘å¼•ç”¨ï¼š

```bibtex
@misc{zhangjun640_chinese_interview,
  author = {zhangjun640},
  title = {Chinese Interview Dataset},
  year = {2025},
  publisher = {Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{[https://huggingface.co/datasets/zhangjun640/Chinese_interview_large](https://huggingface.co/datasets/zhangjun640/Chinese_interview_large)}},
}
```

### è®¸å¯è¯

æœ¬æ•°æ®é›†é‡‡ç”¨ [Apache License 2.0](https://opensource.org/licenses/Apache-2.0) è®¸å¯è¯ã€‚
